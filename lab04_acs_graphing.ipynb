{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: Graphical analysis of the American Community Survey\n",
        "jupyter: python3\n",
        "---\n",
        "\n",
        "\n",
        "The American Community Survey (ACS) is a large survey of households\n",
        "and individuals in the United States, carried out by the US\n",
        "government on a continuous basis (around 3.5 million people are\n",
        "contacted per year).  It is arguably the most authoritative source\n",
        "of information about the demographic composition of the US\n",
        "population, and is used for many purposes in academic research,\n",
        "government, public policy, and in private industry.\n",
        "\n",
        "Some of the questions in the ACS are about sensitive topics, and therefore are only released in aggregate form. The \"public use microsample\" (PUMS) is a set of individual ACS responses that only includes information that has been deemed safe for public release at the individual level. Here we will work with a subset of the ACS/PUMS data.\n",
        "\n",
        "You will need to refer to the documentation to know what the ACS variable names mean: **[ACS PUMS Codebooks](https://www.census.gov/programs-surveys/acs/microdata/documentation/2018.html)** Scroll down for data dictionary 2018 \"1-year\" ACS/PUMS, available in several formats.\n",
        "\n",
        "For this course, we are providing a simplified version of the ACS/PUMS data from 2018. It contains a random subset of the cases and a selected subset of variables.\n",
        "\n",
        "Note that many PUMS variables are described as being \"household\" or \"individual\" variables. These refer to characteristics of households (one or more people living at the same address) or to characteristics of individual people, respectively.\n",
        "\n",
        "As usual we will start by loading our data science libraries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sb\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "try:\n",
        "    df = pd.read_csv(\"./pums_short.csv.gz\")\n",
        "except FileNotFoundError:\n",
        "    df = pd.read_csv(\"./labs/lab04/pums_short.csv.gz\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Getting to know the data\n",
        "\n",
        "\n",
        "As we typically do, find out the size of the data set and what the columns are called."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<details>\n",
        "\n",
        "```\n",
        "print(df.columns)\n",
        "print(df.shape)\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "\n",
        "## Discrete Distributions \n",
        "\n",
        "The `value_counts` method is a useful way to summarize the distribution of a discrete variable. For example, did this survey capture the same number of people across each region?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<details>\n",
        "\n",
        "```\n",
        "df[\"REGION\"].value_counts().sort_index()\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "What did you notice about the distribution of survey respondents across regions? \n",
        "\n",
        "Notice here that the regions are numbered 1-4, which corresponds to the Northeast, Midwest, South, and West (respectively). Even though the regions\n",
        "are numbered, the numbers are unimportant, and so it is important to think\n",
        "of this as categorical data, not numeric. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "sb.barplot(df[\"REGION\"].value_counts())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "What about values that are also categorical, but have a distinct order? For example, let's look at the year that a household living structure was built,\n",
        "`YBL`. Find the distribution of the YBL values.\n",
        "\n",
        "Hint: You can use the `sort_index` method to sort the output of the `value_counts` method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<details>\n",
        "\n",
        "```\n",
        "df[\"YBL\"].value_counts().sort_index()\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "But what do these numbers mean? Let's look at the codebook.\n",
        "\n",
        "\n",
        "| Code  | Description     |\n",
        "| ----- | --------------- | \n",
        "| 1     | 1939 or earlier |\n",
        "| 2     | 1940 to 1949    |\n",
        "| 3     | 1950 to 1959    |\n",
        "| 4     | 1960 to 1969    |\n",
        "| 5     | 1970 to 1979    |\n",
        "| 6     | 1980 to 1989    |\n",
        "| 7     | 1990 to 1999    |\n",
        "| 8     | 2000 to 2004    |\n",
        "| 9     | 2005            |\n",
        "| 10    | 2006            |\n",
        "| 11    | 2007            |\n",
        "| 12    | 2008            |\n",
        "| 13    | 2009            |\n",
        "| 14    | 2010            |\n",
        "| 15    | 2011            |\n",
        "| 16    | 2012            |\n",
        "| 17    | 2013            |\n",
        "| 18    | 2014            |\n",
        "| 19    | 2015            |\n",
        "| 20    | 2016            |\n",
        "| 21    | 2017            |\n",
        "| 22    | 2018            |\n",
        "\n",
        "Despite the fact the numeric codes increase by 1 each time, early codes represent multiple years, including up to an entire decade of construction.\n",
        "However, later codes represent a single year. \n",
        "\n",
        "## Cumulative Proportions\n",
        "\n",
        "Despite the fact that the ordinal variables are not strictly numeric, we can still use them to gauge how recently dwelling structures\n",
        "have been built. We can do this by using the `cumsum` function after ordering the values.\n",
        "\n",
        "We're also going to do a little bit of cleaning to make our lives easier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "data_labels = {\n",
        "    1: \"1939 or earlier\",\n",
        "    2: \"1940 to 1949\",\n",
        "    3: \"1950 to 1959\",\n",
        "    4: \"1960 to 1969\",\n",
        "    5: \"1970 to 1979\",\n",
        "    6: \"1980 to 1989\",\n",
        "    7: \"1990 to 1999\",\n",
        "    8: \"2000 to 2004\",\n",
        "    9: \"2005\",\n",
        "    10: \"2006\",\n",
        "    11: \"2007\",\n",
        "    12: \"2008\",\n",
        "    13: \"2009\",\n",
        "    14: \"2010\",\n",
        "    15: \"2011\",\n",
        "    16: \"2012\",\n",
        "    17: \"2013\",\n",
        "    18: \"2014\",\n",
        "    19: \"2015\",\n",
        "    20: \"2016\",\n",
        "    21: \"2017\",\n",
        "    22: \"2018\",\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "sorted_counts = pd.concat([\n",
        "    df[\"YBL\"].value_counts(),\n",
        "    df[\"YBL\"].value_counts(normalize=True)\n",
        "], axis=1).sort_index().rename(index=data_labels)\n",
        "sorted_counts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now have the human readable labels along with our counts, as well as the percentage of dwellings in each category.\n",
        "\n",
        "Already, we see some interesting information. For example, roughly 13% of ACS respondents live in a house that was built prior to 1939! \n",
        "To get a cumulative look at the data, however, we need to use the `cumsum` function that we saw in lecture."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "sorted_counts.cumsum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Use this table to answer the following questions:\n",
        "\n",
        "1. What percentage of dwellings were built **prior** to 2008?\n",
        "2. Is it possible to accurately say What percentage of dwellings were built **before** 1995? Why or why not?\n",
        "\n",
        "Next, let's see how to this without the preconstructed table.\n",
        "Suppose we ask the same questions, 1 and 2. How could we solve without the cumulative sum?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "(df[\"YBL\"] < 12).mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We used `12` as the value because that group corresponds to 2008 (hint: you need to check the `data_labels` object we constructed from the codebook!). We then\n",
        "take the mean, since for boolean (true/false) values, this is equivalent\n",
        "to finding the proportion that satisfy our condition. The key here is to remember to tie your \n",
        "ordinal group with the correct index in the `data_labels` dictionary.\n",
        "\n",
        "Cumulative proportions have a very important property - the maximum value is **always 1.** This is because **proportions are always between 0 and 1**. \n",
        "Try finding the percentage of dwelllings built before the year 2000, and check your answer against the table above. Remember, if you get a number less\n",
        "than 0 or greater than 1, we know the answer is incorrect.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<details>\n",
        "\n",
        "```\n",
        "(df[\"YBL\"] <= 7).mean()\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "Now try finding the opposite - the percentage of dwellings built after and including 2008. Hint: the opposite of \">\" is \"<=\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<details>\n",
        "\n",
        "```\n",
        "(df[\"YBL\"] > 7).mean()\n",
        "\n",
        "# also \n",
        "(df[\"YBL\"] >= 8).mean()\n",
        "\n",
        "# and also\n",
        "1 - (df[\"YBL\"] <= 7).mean()\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "## Boxplots and histograms\n",
        "\n",
        "A boxplot is a way to visualize a distribution. Below we create a boxplot for the household income variable in the ACS."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "sb.boxplot(x = \"HINCP\", data = df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The data are very skewed, which is a common property of income data.  Skew is not a bad thing, but it is an important characteristic of the data to be aware of.\n",
        "\n",
        "Let's zoom in on the box:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "sb.boxplot(x = \"HINCP\", showfliers = False, data=df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Use the box plot above to describe the **central tendancy** (middle of the data) and **spread** (how tighly clustered are the data around the middle) and **skew** (the amount of spread on each side of the central tendency)\n",
        "of household income in this data set.\n",
        "\n",
        "*Double click to edit*\n",
        "\n",
        "Create a boxplot for the `NP` variable. Describe the central tendency, spread, and skew."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<details>\n",
        "\n",
        "```\n",
        "sb.boxplot(data = df, x = \"NP\")\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "A histogram is another way to visualize a distribution. It requires more data to produce an accurate histogram than it does to produce an accurate boxplot, but in this case we have plenty of data so either approach is useful.  There are various ways to decide how many bins to use when creating a histogram.  Here we set this value manually to have 20 bins."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "sb.histplot(data = df, x = \"HINCP\", bins = 20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create a histogram with 100 bins. What changed in your plot? Which plot do your prefer?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<details>\n",
        "\n",
        "```\n",
        "sb.histplot(data = df, x = \"HINCP\", bins = 100)\n",
        "```\n",
        "</details>\n",
        "\n",
        "In the above histograms we notice **strong right skew**. One way to tame skew is to a **logarithmic transformation**. A logarithm solves $b^c = a$ for $c$. We write $\\log_b(a) = c$. Here, we'll use $b = 2$. The `numpy` library has a function we can use to compute the base two logarithms.\n",
        "\n",
        "Let's log transform the data. Logs are not defined for non-postive values, so we will replace (or \"clip\") data below 1 to have the value 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df[\"logHINCP\"] = np.log2(df[\"HINCP\"].clip(1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Log transformations help tame skew. Skew isn't necessarily bad and is not something that always must be removed.  But in general it is easier to analyze data that has been transformed to reduce the skew.\n",
        "\n",
        "Create a box plot for the log transformed data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<details>\n",
        "\n",
        "```\n",
        "sb.boxplot(data = df, x = \"logHINCP\")\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "Now create a histogram of the log transformed income data. Use 100 bins. Write a few words describing the central tendency, spread and skew visible in this plot."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<details>\n",
        "\n",
        "```\n",
        "sb.histplot(data = df, x = \"logHINCP\", bins = 100)\n",
        "```\n",
        "\n",
        "</details>\n",
        "    \n",
        "In the plot we see the central tendency is around 15 to 17 (on the log base 2 dollar scale). Most observations fall in teh range 10 to 20. There is not much skew, but we do see a spike at 0 that represents the observations we clipped to 1. (Why is is $\\log_b(1) = 0$ for any $b$?)\n",
        "    \n",
        "\n",
        "\n",
        "## Cumulative Proportions \n",
        "\n",
        "Previously, we saw a cumulative distribution for ordinal variables. However,\n",
        "cumulative proportions can be created for continuous variables as well, and in exactly the same way.\n",
        "To demonstrate, let's look at income data again. What percentage of households had an\n",
        "income greater than $100,000?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "(df[\"HINCP\"] >= 100000).mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "What percentage of households had an income **between** $25,000 and $75,000?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<details>\n",
        "\n",
        "```\n",
        "(df[\"HINCP\"].between(25000, 75000)).mean()\n",
        "# or \n",
        "((df[\"HINCP\"] >= 25000) & (df[\"HINCP\"] <= 75000)).mean()\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "\n",
        "## Cumulative Proportions and Groups\n",
        "\n",
        "Some of the most interesting questions in data science arise from proportions and means within groups\n",
        "of interest. To review grouping, let's start with a simple  question: what's the average number of people in a household\n",
        "in each region? Try it on your own, and then check your answer below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<details>\n",
        "\n",
        "```\n",
        "df.groupby(\"REGION\")[\"NP\"].mean()\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "There is a small difference, but it may seem\n",
        "unclear whether this is the result is different enough\n",
        "to say that there is a real or practical difference across the regions. Keep this in mind - we're going to talk about this later in the course!\n",
        "\n",
        "For now though, after recalling how we do groupings, let's ask a new question: is the proportion of \n",
        "households with an income greater than $100,000 different across regions? This is a slightly different question, as it\n",
        "requires cumulative proportions within a region. With a bit \n",
        "of adjusted code, we can aswer this question."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "regions = {1: \"Northeast\", 2: \"Midwest\", 3: \"South\", 4: \"West\"}\n",
        "\n",
        "income_proportions = (\n",
        "    (df[\"HINCP\"] >= 100000) # create boolean values for income >= $100k/yr\n",
        "    .groupby(df[\"REGION\"])  # next, group by the region\n",
        "    .mean() # take the mean of the boolean values for each region\n",
        "    .rename(regions) # rename the regions to a human-readble names\n",
        ")\n",
        "print(income_proportions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notice the differences with the grouping code from earlier,\n",
        "but this style still works. Here, we pass the grouping\n",
        "variable as an argument to the groupby method instead\n",
        "of calling it directly on the dataframe, but the results \n",
        "are the same.\n",
        "\n",
        "Writing the code for these styles of queries can be tricky, so feel free to ask CoPilot for help! \n",
        "\n",
        "When we run this, we see that the proportions are indeed different across the regions. Let's try another one: does the age of dwellings differ across regions? What proportions of dwellings were built BEFORE 1939 in each region?\n",
        "\n",
        "<details>\n",
        "\n",
        "```\n",
        "(\n",
        "    (df[\"YBL\"] == 1) \n",
        "    .groupby(df[\"REGION\"])  \n",
        "    .mean() \n",
        "    .rename(regions) \n",
        ")\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "You should be a big difference between the Northeast and Midwest vs. the South and the West. Think about the history of the US - does the data match \n",
        "your intuition?\n",
        "\n",
        "Practice a bit on your own with cumulative proportions and grouping, and\n",
        "try to use variables we haven't explored yet. \n",
        "Feel free to look through the codebook to find a variable that interests you. If you stumble on the coding syntax,\n",
        "don't hesitate to ask for help! Pandas is a powerful tool, but it can be tricky to learn at first."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}